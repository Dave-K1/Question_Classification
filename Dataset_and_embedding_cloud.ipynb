{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path_in = './data/'\n",
    "\n",
    "train = pd.read_csv(path_in + 'train.csv')\n",
    "#test = pd.read_csv(path_in + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = {}\n",
    "\n",
    "with open(\"./data/GloVe300d.txt\") as f:\n",
    "    for line in f:\n",
    "        row = line.split()\n",
    "        key = row[0]\n",
    "        val = row[1:301]            \n",
    "        embedding[key] = val \n",
    "        \n",
    "#pickle.dump(embedding, \"./data/GloVe50d.p\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/GloVe50d.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4afca14d62e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Load the saved embedding GloVe with dimension 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/GloVe50d.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/GloVe50d.p'"
     ]
    }
   ],
   "source": [
    "## Load the saved embedding GloVe with dimension 50\n",
    "embedding = pickle.load(open(\"./data/GloVe50d.p\", \"rb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embedding layer\n",
    "\n",
    "Layer to allow feeding a sentence. The layer splits the sentence up and extracts the weights from the GloVe embedding. \n",
    "Note that not all words are in the dictionary. The layer truncates sentences with more than 45 words/punctuation.\n",
    "It returns the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In: sentence\n",
    "## Out: tensor with dimension [seq_len, 1, dim]\n",
    "\n",
    "def get_weights(sentence, glove):\n",
    "    ## Parameter\n",
    "    embedding_dimension = 300\n",
    "    sentence_max_length = 45\n",
    "    \n",
    "    ## Convert to lowercase and split sentence\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    ## Sepearate punctuation\n",
    "    chs = string.punctuation\n",
    "    for ch in chs:\n",
    "        idx = sentence.find(ch)\n",
    "        \n",
    "        if idx != -1:\n",
    "            sentence = sentence.replace(ch, \" \" + ch)\n",
    "            \n",
    "    sentence = sentence.split(' ') \n",
    "         \n",
    "     ## Truncate\n",
    "    sentence = sentence[0:sentence_max_length]\n",
    "\n",
    "    ## Find weights\n",
    "    matrix_len = len(sentence)\n",
    "    weights_matrix = np.zeros((matrix_len, embedding_dimension))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(sentence):\n",
    "        try: \n",
    "            weights_matrix[i] = glove[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dimension, ))\n",
    "\n",
    "    #print(\"{} / {} words found\".format(words_found, len(sentence)))    \n",
    "    \n",
    "    ## Pad with zeros. Longest sentence in data has 44 words. Pad to 45.\n",
    "    z = np.zeros((1, embedding_dimension))\n",
    "    \n",
    "    for i in range(sentence_max_length-matrix_len):      \n",
    "        weights_matrix = np.concatenate((weights_matrix, z), axis=0)\n",
    "    \n",
    "    ## Convert to torch tensor\n",
    "    weights_matrix = torch.tensor(weights_matrix)\n",
    "    weights_matrix = weights_matrix.view(sentence_max_length, -1, embedding_dimension)    \n",
    "    weights_matrix = weights_matrix.float()\n",
    "    \n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataloader function to extract the sentence and the label from the dataset by index\n",
    "def get_train_data(sample):  \n",
    "    sentence = train['question_text'].iloc[sample]\n",
    "    label = train['target'].iloc[sample]\n",
    "    label = torch.tensor(label).float()\n",
    "    label.unsqueeze_(-1)\n",
    "    \n",
    "    return sentence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test: custom dataset\n",
    "class QuestionsDataset(Dataset):\n",
    "    '''Questions dataset'''\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to file with questions, question ids and labels\n",
    "        \"\"\"\n",
    "\n",
    "        self.traindata = pd.read_csv(csv_file)\n",
    "        self.embedding = embedding = pickle.load(open(\"./data/GloVe300d.p\", \"rb\") )\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.traindata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "            ## Parameter\n",
    "            embedding_dimension = 300\n",
    "            sentence_max_length = 45\n",
    "            \n",
    "            sentence = traindata['question_text'].iloc[idx]\n",
    "            label = traindata['target'].iloc[idx]\n",
    "            label = torch.tensor(label).float()\n",
    "            label.unsqueeze_(-1)\n",
    "    \n",
    "            ## Convert to lowercase and split sentence\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "            ## Sepearate punctuation\n",
    "            chs = string.punctuation\n",
    "            for ch in chs:\n",
    "                idx = sentence.find(ch)\n",
    "\n",
    "                if idx != -1:\n",
    "                    sentence = sentence.replace(ch, \" \" + ch)\n",
    "\n",
    "            sentence = sentence.split(' ') \n",
    "\n",
    "             ## Truncate\n",
    "            sentence = sentence[0:sentence_max_length]\n",
    "\n",
    "            ## Find weights\n",
    "            matrix_len = len(sentence)\n",
    "            weights_matrix = np.zeros((matrix_len, embedding_dimension))\n",
    "            words_found = 0\n",
    "\n",
    "            for i, word in enumerate(sentence):\n",
    "                try: \n",
    "                    weights_matrix[i] = glove[word]\n",
    "                    words_found += 1\n",
    "                except KeyError:\n",
    "                    weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dimension, ))\n",
    "\n",
    "            #print(\"{} / {} words found\".format(words_found, len(sentence)))    \n",
    "\n",
    "            ## Pad with zeros. Longest sentence in data has 44 words. Pad to 45.\n",
    "            z = np.zeros((1, embedding_dimension))\n",
    "\n",
    "            for i in range(sentence_max_length-matrix_len):      \n",
    "                weights_matrix = np.concatenate((weights_matrix, z), axis=0)\n",
    "\n",
    "            ## Convert to torch tensor\n",
    "            weights_matrix = torch.tensor(weights_matrix)\n",
    "            weights_matrix = weights_matrix.view(sentence_max_length, -1, embedding_dimension)    \n",
    "            weights_matrix = weights_matrix.float() \n",
    "        \n",
    "        \n",
    "        \n",
    "        return weights_matrix, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net\n",
    "\n",
    "Takes a sentence as input using the custom function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer_1):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_layer_1)       \n",
    "        self.fc1 = nn.Linear(hidden_layer_1, 1)\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sentence, embedding):        \n",
    "        ## Convert sentence into tensor:\n",
    "        x = get_weights(sentence, embedding)\n",
    "        \n",
    "        ## Network\n",
    "        output, (h_out, _) = self.lstm(x)        \n",
    "        output = output[-1, :, :]\n",
    "        output.squeeze_()     \n",
    "        x = self.out(self.fc1(output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 20\n",
      "Loss: 0.238948038039729 | Training accuracy: 0.4679262699999999\n",
      "Epoch 2 / 20\n",
      "Loss: 0.2373404067995958 | Training accuracy: 0.46795828999999994\n",
      "Epoch 3 / 20\n"
     ]
    }
   ],
   "source": [
    "### Function for training\n",
    "epochs = 20\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "hidden_layers = 150\n",
    "num_features = 300\n",
    "train_size = 10000\n",
    "\n",
    "#net = Net(num_features, hidden_layers)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "## Iterate through all data\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch {} / {}\".format(epoch+1, epochs))\n",
    "    loss_sub =[]\n",
    "    acc_sub = []    \n",
    "    correct = 0\n",
    "           \n",
    "    for sample in range(len(train[0:train_size])): \n",
    "\n",
    "        sentence, label = get_train_data(sample)\n",
    "        \n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = net(sentence, embedding)\n",
    "        \n",
    "        # Evaluate output / compute loss\n",
    "        loss = criterion(output, label)       \n",
    "        \n",
    "        # Backward pass / optimize\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "        ## Evaluate\n",
    "        output = np.where(output.detach().numpy() > 0.5, 1, 0)\n",
    "        correct += (output == label.numpy()).sum()\n",
    "        acc = correct / train_size\n",
    "        acc_sub.append(acc)\n",
    "        \n",
    "        loss_sub.append(loss.item())\n",
    "             \n",
    "    acc_list.append(np.mean(acc_sub))   \n",
    "    loss_list.append(np.mean(loss_sub)) \n",
    "    \n",
    "    #print(loss_list[-1])\n",
    "    print(\"Loss: {} | Training accuracy: {}\".format(loss_list[-1], acc_list[-1]))\n",
    "\n",
    "    \n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.linspace(0, len(train), len(train)+1)\n",
    "random.shuffle(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
