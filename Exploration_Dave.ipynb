{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path_in = './data/'\n",
    "\n",
    "train = pd.read_csv(path_in + 'train.csv')\n",
    "#test = pd.read_csv(path_in + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure: Question ID (qid), question text, target.\n",
    "\n",
    "Target 0 - Negative\n",
    "\n",
    "Target 1 - Positive\n",
    "\n",
    "Training data:\n",
    "\n",
    "Around 1,300,000 samples. Will have to be split up into training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for training: 1306122\n",
      "Ratio positives to total: 0.06\n"
     ]
    }
   ],
   "source": [
    "## Train data\n",
    "print('Number of samples for training: {}\\nRatio positives to total: {:.2f}'.format(len(train), \n",
    "                                                                                    len(train.loc[train['target'] == 1])/len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example sentence\n",
    "\n",
    "train['question_text'][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "As a start, the 'glove.840B.300d' embedding is used.\n",
    "\n",
    "Also, you can use 'torchtext.vocab.GloVe(name='840B', dim=300)' (see [torchtext package](https://torchtext.readthedocs.io/en/latest/vocab.html#vocab) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the vectors\n",
    "\n",
    "path = r'./data/embeddings/'\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(path + 'glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:])\n",
    "        embeddings_dict[word] = vector\n",
    "        \n",
    "## Save to disc as pickle\n",
    "#pickle.dump( embeddings_dict, open( \"./data/embeddings/GloVe.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved embedding GloVe with dimension 50\n",
    "embedding = pickle.load(open(\"./data/embeddings/GloVe50d.p\", \"rb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate input vector for neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word in dataset’s vocabulary, we check if it is on GloVe’s vocabulary. If it is, we load its pre-trained word vector. Otherwise, we initialize a random vector.\n",
    "\n",
    "Generate a matrix with dimensions:\n",
    "\n",
    "(dataset’s vocabulary length, word vectors dimension)\n",
    "\n",
    "len(target_vocab): Length of the longest sentence? Pad the rest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take sentence as list of words and get weights from embedding.\n",
    "# Pad with trailing 0s if neccessary\n",
    "\n",
    "# TODO: Exctract punctuation\n",
    "\n",
    "\n",
    "def get_weights(sentence):\n",
    "    ## Split the sentence into words and count them\n",
    "\n",
    "    print(\"Sentence:\\n\" + str(sentence) + \"\\n\")\n",
    "    \n",
    "    sentence = sentence.split(' ')\n",
    "        \n",
    "    glove = embedding\n",
    "    emb_dim = 50\n",
    "\n",
    "    # Find weights<\n",
    "    matrix_len = len(sentence)\n",
    "    weights_matrix = np.zeros((matrix_len, 50))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(sentence):\n",
    "        try: \n",
    "            weights_matrix[i] = glove[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "\n",
    "    print(\"{} / {} words found\".format(words_found, len(sentence)))\n",
    "    \n",
    "    \n",
    "    ## Pad with zeros. Longest sentence in data has 44 words. Pad to 45.\n",
    "    z = np.zeros((1, 50))\n",
    "    \n",
    "    for i in range(45-matrix_len):      \n",
    "        weights_matrix = np.concatenate((weights_matrix, z), axis=0)\n",
    "         \n",
    "    \n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c18eca32651a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "sentence = train['question_text'][10]\n",
    "\n",
    "weights = get_weights(sentence)\n",
    "\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net\n",
    "\n",
    "The input dimension for the net are 45 x 50.\n",
    "\n",
    "Use LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Net\n",
    "### Define net for test\n",
    "class Net(nn.Module):   \n",
    "    def __init__(self, seq_len, batch_size, hidden_size, input_size, D_out):\n",
    "        super().__init__()\n",
    "        ## Embedding layer\n",
    "        #self.emb = nn.Embedding(output_size, hidden_size)\n",
    "               \n",
    "        ## Layer 1\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)      # Add number of layers         \n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        ## Output layer\n",
    "        #n_hidden_1 = hidden_size * 1\n",
    "        \n",
    "        #self.fc2 = nn.Linear(n_hidden_1, D_out)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "            \n",
    "    def forward(self, x): \n",
    "        \n",
    "        #h0 = (num_layers * num_directions, batch, hidden_size)\n",
    "        #c0 = (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        ## First layer \n",
    "        y = self.lstm1(x)\n",
    "        \n",
    "        #y = self.relu1(y)\n",
    "    \n",
    "        ## Output layer\n",
    "        #y = self.out_act(self.fc2(y))\n",
    "        y = self.out_act(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, model, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    loss_accuracy = {'training_loss':[], 'validation_accuracy':[]}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #clear_output(wait=True)\n",
    "        print(\"Epoch {} / {}\\n=============\".format(epoch+1, epochs))\n",
    "            \n",
    "        if epoch > 0:\n",
    "            print(\"Training loss: {}\\nValidation accuracy: {:.2f}%\".format(loss.item(), accuracy))\n",
    "            pass\n",
    "        \n",
    "        # Add loop for processing batches\n",
    "        #for x in X:\n",
    "\n",
    "        x = X\n",
    "        y = Y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "                ## Forward pass\n",
    "            #print(x.shape)\n",
    "\n",
    "        yhat = model(x)\n",
    "\n",
    "        print(yhat.shape)\n",
    "                ## Compute loss\n",
    "        loss = criterion(yhat, y)\n",
    "                ## Compute gradient in backward pass\n",
    "        loss.backward()\n",
    "                ## Update weights\n",
    "        optimizer.step() \n",
    "\n",
    "        loss_accuracy['training_loss'].append(loss.item())\n",
    "             \n",
    "    print(\"Training complete!\")\n",
    "                \n",
    "    return loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Net(\n",
      "  (lstm1): LSTM(50, 1, batch_first=True)\n",
      "  (relu1): ReLU()\n",
      "  (out_act): Sigmoid()\n",
      ")>\n",
      "Epoch 1 / 2\n",
      "=============\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sigmoid(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-99cdbdc4495e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m## Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mloss_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-a442dd166427>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(X, Y, model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-5a5a6021a446>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m## Output layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#y = self.out_act(self.fc2(y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sigmoid(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "learning_rate = 0.1\n",
    "\n",
    "seq_len = 45\n",
    "batch_size = 45\n",
    "input_size = 50\n",
    "hidden_size = 1\n",
    "D_out = 1\n",
    "\n",
    "data = torch.from_numpy(weights)\n",
    "\n",
    "## Load data\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=data, batch_size=45, shuffle=True)\n",
    "#validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "\n",
    "#data = data.transpose(0,1)\n",
    "\n",
    "data = data.view(-1, 45, 50)\n",
    "data = data.float()\n",
    "\n",
    "y = torch.tensor(1).view(-1,1)\n",
    "\n",
    "## Initialize model\n",
    "net = Net(seq_len, batch_size, hidden_size, input_size, D_out)\n",
    "#net.apply(weights_init)\n",
    "print(net.parameters)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "## Train the model\n",
    "loss_accuracy = fit(data, y, net, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 45])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = torch.from_numpy(weights)\n",
    "#data = data.transpose(0,1)\n",
    "data = data.view(-1, 50, 45)\n",
    "#data = data.float()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0403, -0.0799,  0.8967,  ..., -0.6373,  1.2048,  0.6531],\n",
       "         [ 0.4826,  0.0020, -0.6749,  ...,  0.1296,  0.2725, -0.3295],\n",
       "         [ 0.3421,  0.1561,  0.2095,  ..., -0.8044,  0.1839, -0.3433],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ToyNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers):\n",
    "        super(self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        return self.gru(self.embedding(inp), hidden)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
