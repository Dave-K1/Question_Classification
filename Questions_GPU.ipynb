{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Embedding\n",
    "\n",
    "TODOs:\n",
    "- Split training data into train, val, test\n",
    "- Shuffle training data / Train in batches -> Use dataloader\n",
    "\n",
    "\n",
    "- Train on more samples\n",
    "\n",
    "\n",
    "- Schedule learning rates (optional)\n",
    "\n",
    "\n",
    "- Comments and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams.update({'font.size': 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda = torch.device('cuda')\n",
    "    x_cpu = torch.empty(2)\n",
    "    x_gpu = torch.empty(2, device=cuda)\n",
    "    x_cpu_long = torch.empty(2, dtype=torch.int64)\n",
    "    print(\"Cuda available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path_in = './data/'\n",
    "\n",
    "train = pd.read_csv(path_in + 'train.csv')\n",
    "#test = pd.read_csv(path_in + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load embedding and save to pickle\n",
    "embedding = {}\n",
    "\n",
    "with open(\"./data/GloVe300d.txt\") as f:\n",
    "    for line in f:\n",
    "        row = line.split()\n",
    "        key = row[0]\n",
    "        val = row[1:301]            \n",
    "        embedding[key] = val\n",
    "        \n",
    "with open('./data/GloVe300d.p', 'wb') as handle:\n",
    "    pickle.dump(embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved embedding GloVe with dimension 50\n",
    "embedding = pickle.load(open(\"./data/GloVe300d.p\", \"rb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data exploration\n",
    "\n",
    "There are over 1 300 000 sentences in the dataset. Let's see how long the sentences are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceLength(sentence):\n",
    "    length = len(sentence.split())    \n",
    "    \n",
    "    return length\n",
    "\n",
    "sentence_lengths = []\n",
    "for i in range(len(train)):\n",
    "    sentence_lengths.append(getSentenceLength(train[\"question_text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sentence_lengths, range=(0, 30), color=\"b\")\n",
    "plt.title(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words per sentence can be found?\n",
    "\n",
    "Sample 30 sentences and compute statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberofWordsFound():\n",
    "    ## Generate random\n",
    "    population = list(np.arange(0, len(train)))\n",
    "    population = random.sample(population, 10000)\n",
    "    \n",
    "    number_of_words = []\n",
    "    \n",
    "    for sample in population:\n",
    "        sentence = train['question_text'].iloc[int(sample)]\n",
    "\n",
    "        ## Convert to lowercase and split sentence\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        ## Sepearate punctuation\n",
    "        chs = string.punctuation\n",
    "        for ch in chs:\n",
    "            idx = sentence.find(ch)\n",
    "\n",
    "            if idx != -1:\n",
    "                sentence = sentence.replace(ch, \" \" + ch)\n",
    "\n",
    "        sentence = sentence.split(' ') \n",
    "        number_of_words.append(len(sentence))\n",
    "        \n",
    "    return number_of_words\n",
    "\n",
    "\n",
    "num_of_words = getNumberofWordsFound()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(num_of_words, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do: fit curve\n",
    "print(np.mean(num_of_words))\n",
    "print(np.std(num_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Generate weight matrix\n",
    "Target dimension: [sequence len, sample, dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 45\n",
    "dimension = 300\n",
    "number_samples = 50000\n",
    "\n",
    "\n",
    "### Initialize matrix\n",
    "weights = np.zeros((seq_len, number_samples, dimension))\n",
    "labels = []\n",
    "\n",
    "### Extract weight matrix for each sample\n",
    "for sample in range(number_samples):\n",
    "    sentence = train['question_text'].iloc[sample]\n",
    "    label = train['target'].iloc[sample]\n",
    "    labels.append(label)\n",
    "       \n",
    "    ## Convert to lowercase and split sentence\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    ## Sepearate punctuation\n",
    "    chs = string.punctuation\n",
    "    for ch in chs:\n",
    "        idx = sentence.find(ch)\n",
    "        \n",
    "        if idx != -1:\n",
    "            sentence = sentence.replace(ch, \" \" + ch)\n",
    "            \n",
    "    sentence = sentence.split(' ')   \n",
    "    \n",
    "     ## Truncate\n",
    "    sentence = sentence[0:seq_len]\n",
    "\n",
    "    ## Find weights\n",
    "    matrix_len = len(sentence)\n",
    "    weights_matrix = np.zeros((matrix_len, dimension))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(sentence):\n",
    "        try: \n",
    "            weights_matrix[i] = embedding[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(dimension, ))\n",
    "   \n",
    "    \n",
    "    ## Pad with zeros to seq_len\n",
    "    z = np.zeros((1, dimension))\n",
    "    \n",
    "    for i in range(seq_len-matrix_len):      \n",
    "        weights_matrix = np.concatenate((weights_matrix, z), axis=0)\n",
    "     \n",
    "    weights[:, sample, :] = weights_matrix\n",
    "\n",
    "print(\"Dimension of weight matrix: {}\".format(weights.shape))    \n",
    "print(\"Ratio positives/total: {}\".format(labels.count(1)/number_samples))\n",
    "\n",
    "\n",
    "## Convert to torch tensors\n",
    "weights = torch.tensor(weights)\n",
    "weights = weights.float()  \n",
    "\n",
    "labels = torch.tensor(labels).float()\n",
    "\n",
    "\n",
    "## Convert to GPU if cuda is available\n",
    "# if torch.cuda.is_available():\n",
    "#     weights = weights.to(cuda)\n",
    "#     labels = labels.to(cuda)\n",
    "#     print(\"Tensors converted to cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define dataloader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        ## Load full weight matrix\n",
    "        seq_len = 45\n",
    "        labels = []\n",
    "        dimension = 300\n",
    "        \n",
    "        for sample in range(len(train)):\n",
    "            sentence = train['question_text'].iloc[sample]\n",
    "            label = train['target'].iloc[sample]\n",
    "            labels.append(label)\n",
    "\n",
    "            ## Convert to lowercase and split sentence\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "            ## Sepearate punctuation\n",
    "            chs = string.punctuation\n",
    "            for ch in chs:\n",
    "                idx = sentence.find(ch)\n",
    "\n",
    "                if idx != -1:\n",
    "                    sentence = sentence.replace(ch, \" \" + ch)\n",
    "\n",
    "            sentence = sentence.split(' ')   \n",
    "\n",
    "             ## Truncate\n",
    "            sentence = sentence[0:seq_len]\n",
    "\n",
    "            ## Find weights\n",
    "            matrix_len = len(sentence)\n",
    "            weights_matrix = np.zeros((matrix_len, dimension))\n",
    "            words_found = 0\n",
    "\n",
    "            for i, word in enumerate(sentence):\n",
    "                try: \n",
    "                    weights_matrix[i] = embedding[word]\n",
    "                    words_found += 1\n",
    "                except KeyError:\n",
    "                    weights_matrix[i] = np.random.normal(scale=0.6, size=(dimension, ))\n",
    "\n",
    "\n",
    "            ## Pad with zeros to seq_len\n",
    "            z = np.zeros((1, dimension))\n",
    "\n",
    "            for i in range(seq_len-matrix_len):      \n",
    "                weights_matrix = np.concatenate((weights_matrix, z), axis=0)\n",
    "\n",
    "            self.weights[:, sample, :] = weights_matrix\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.weights)\n",
    "    \n",
    "    \n",
    "    def __getitem(self, idx):\n",
    "        self.sample = weights[sample]\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layer_1):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=hidden_layer_1)       \n",
    "        self.fc1 = nn.Linear(hidden_layer_1, 1)\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):        \n",
    "        output, (h_out, _) = self.lstm(x)        \n",
    "        output = output[-1, :, :]\n",
    "        output.squeeze_()     \n",
    "        x = self.out(self.fc1(output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for training\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "hidden_layers = 300\n",
    "train_size = 50000\n",
    "\n",
    "\n",
    "net = Net(hidden_layers)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "## Iterate through all data\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch {} / {}\".format(epoch+1, epochs))\n",
    "    loss_sub =[]\n",
    "    acc_sub = []\n",
    "      \n",
    "    correct = 0\n",
    "    \n",
    "        \n",
    "    for sample in range(train_size):                 \n",
    "        ## Get training data\n",
    "        x = weights[:, sample, :]      \n",
    "        x.unsqueeze_(1)\n",
    "        y = labels[sample]\n",
    "        y = y.view(1)\n",
    "        \n",
    "        # Set gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = net(x)\n",
    "        \n",
    "        # Evaluate output / compute loss\n",
    "        loss = criterion(output, y)       \n",
    "        \n",
    "        # Backward pass / optimize\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "        ## Evaluate\n",
    "        output = np.where(output.detach().numpy() > 0.5, 1, 0)\n",
    "        correct += (output == y.numpy()).sum()\n",
    "        \n",
    "    acc = correct / train_size\n",
    "    acc_sub.append(acc)\n",
    "        \n",
    "    loss_sub.append(loss.item())\n",
    "          \n",
    "        \n",
    "    acc_list.append(np.mean(acc_sub))   \n",
    "    loss_list.append(np.mean(loss_sub)) \n",
    "    \n",
    "    #print(loss_list[-1])\n",
    "    print(\"Loss: {} | Training accuracy: {}\".format(loss_list[-1], acc_list[-1]))\n",
    "\n",
    "    \n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.plot(loss_accuracy['validation_accuracy'], color=\"blue\", label=\"validation\")\n",
    "plt.plot(loss_accuracy['training_accuracy'], color=\"gray\", label=\"training\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "\n",
    "\n",
    "fig = plt.figure(2)\n",
    "plt.plot(loss_accuracy['training_loss'], color=\"red\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss [-]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
