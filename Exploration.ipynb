{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path_in = './data/'\n",
    "\n",
    "train = pd.read_csv(path_in + 'train.csv')\n",
    "test = pd.read_csv(path_in + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure: Question ID (qid), question text, target.\n",
    "\n",
    "Target 0 - Negative\n",
    "\n",
    "Target 1 - Positive\n",
    "\n",
    "Training data:\n",
    "\n",
    "Around 1,300,000 samples. Will have to be split up into training, validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for training: 1306122\n",
      "Ratio positives to total: 0.06\n"
     ]
    }
   ],
   "source": [
    "## Train data\n",
    "print('Number of samples for training: {}\\nRatio positives to total: {:.2f}'.format(len(train), \n",
    "                                                                                len(train.loc[train['target'] == 1])/len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How did Otto von Guericke used the Magdeburg hemispheres?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example sentence\n",
    "train['question_text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "000042bf85aa498cd78e\n"
     ]
    }
   ],
   "source": [
    "print(train['target'][3])\n",
    "print(train['qid'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Currently, the questions are all in words that a computer will have problems handling. We have to first process the questions by tokenizing them. For example \"One fish, two fish, red fish blue fish\" -> [1 2 3 2 4 2 5 2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tokenizer dictionary in the tokenizer class\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['question_text'])\n",
    "tokenizer.fit_on_texts(test['question_text'])\n",
    "\n",
    "# Split train set into train and validation sets\n",
    "train, validation = train_test_split(train, test_size=0.1)\n",
    "\n",
    "# Tokenize the questions\n",
    "train_sequences = tokenizer.texts_to_sequences(train['question_text'])\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation['question_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['question_text'])\n",
    "\n",
    "# Save the tokenizer dictionary and the number of words in it\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4d73e8b48ede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "training_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(train_sequences)).int(), torch.tensor(np.array(train['target'])).float32())\n",
    "validation_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(validation_sequences)).int(), torch.tensor(np.array(validation['target'])).float32())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ce0662d50325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;31m# assuming trailing dimensions and type of all the Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;31m# in sequences are same and fetching those from sequences[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0mtrailing_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train_sequences = torch.nn.utils.rnn.pad_sequence(train_sequences)\n",
    "\n",
    "torch.IntTensor(train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "As a start, the 'glove.6B.50d' embedding is used.\n",
    "\n",
    "Also, you can use 'torchtext.vocab.GloVe(name='840B', dim=300)' (see [torchtext package](https://torchtext.readthedocs.io/en/latest/vocab.html#vocab) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_File = './data/embeddings/glove.6B.50d/glove.6B.50d.txt'\n",
    "embeddings_dim = 50\n",
    "embeddings_index = {}\n",
    "\n",
    "print('Loading word embeddings...')\n",
    "\n",
    "with open(embeddings_File) as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, embeddings_dim));\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;  \n",
    "\n",
    "print('Finished loading word embeddings. {:.0f} words loaded.'.format(len(embeddings_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "\n",
    "Explore the data. Find for example the longest quora question.\n",
    "\n",
    "Pad the data with empty or null words\n",
    "\n",
    "Embed the data\n",
    "\n",
    "Create an RNN network to train the data\n",
    "\n",
    "Look at the results and iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias,0)\n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_hidden_1, D_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize word embeddings layer using weights\n",
    "        weight = torch.FloatTensor(embeddings_matrix)\n",
    "        embeds = nn.Embedding.from_pretrained(weight)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embedding_dim, n_hidden_1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(n_hidden_2, D_out)\n",
    "        self.out_act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embeds_out = self.embeds(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        hidden = None\n",
    "        lstm_out, _ = self.lstm1(embeds_out, hidden)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        fc1_out = self.relu1(self.fc1(lstm_out))\n",
    "        \n",
    "        # Output layer\n",
    "        y = self.out_act(self.fc2(fc1_out))\n",
    "        \n",
    "        return y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_set, model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    loss_accuracy = {'training_loss':[], 'validation_accuracy':[], 'validation_precision':[], 'validation_recall':[]}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Epoch {} / {}\\n=============\".format(epoch+1, epochs))\n",
    "            \n",
    "        if epoch > 0:\n",
    "            print(\"Training loss: {}\\nValidation accuracy: {:.2f}%\".format(loss.item(), accuracy))\n",
    "            pass\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            ## Forward pass\n",
    "            yhat = model(x)\n",
    "            ## Compute loss\n",
    "            loss = criterion(yhat, y)\n",
    "            ## Compute gradient in backward pass\n",
    "            loss.backward()\n",
    "            ## Update weights\n",
    "            optimizer.step() \n",
    "            \n",
    "            loss_accuracy['training_loss'].append(loss.item())\n",
    "         \n",
    "        ## Compute validation accuracy\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for x, y in validation_loader:\n",
    "            yhat = net(x)           \n",
    "            yhat = np.where(yhat.detach().numpy() > 0.5, 1, 0)\n",
    "            correct = (yhat == y.detach().numpy()).sum()                        \n",
    "            accuracy = 100 * (correct / validation_loader.batch_size)\n",
    "            \n",
    "        loss_accuracy['validation_accuracy'].append(accuracy)       \n",
    "        model.train()\n",
    "        \n",
    "        ## Add precision and recall\n",
    "        \n",
    "    print(\"Training complete!\")\n",
    "                \n",
    "    return loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "## Network dimensions\n",
    "D_in = embeddings_dim\n",
    "n_hidden_1 = 16\n",
    "n_hidden_2 = 16\n",
    "D_out = 1\n",
    "\n",
    "## Load data\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=8000, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "\n",
    "## Initialize model\n",
    "net = Net(D_in, n_hidden_1, n_hidden_2, D_out)\n",
    "net.apply(weights_init)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "## Train the model\n",
    "loss_accuracy = train(training_dataset, net, criterion, train_loader, validation_loader, optimizer, epochs=epochs)\n",
    "\n",
    "\n",
    "## Plots\n",
    "fig = plt.figure(1)\n",
    "plt.plot(loss_accuracy['training_loss'], color=\"red\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss [-]\")\n",
    "\n",
    "fig = plt.figure(2)\n",
    "plt.plot(loss_accuracy['validation_accuracy'], color=\"blue\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
